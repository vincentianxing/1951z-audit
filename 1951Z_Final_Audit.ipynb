{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Group Member: Jessica Yang, Vincent Zhu"
      ],
      "metadata": {
        "id": "GYHTp_sk6EtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "!pip install interpret --quiet\n",
        "!pip install lightgbm --quiet\n",
        "!pip install shap --quiet\n",
        "!pip install lime --quiet"
      ],
      "metadata": {
        "id": "9a9auuMmJKmx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "import requests\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "# from folktables import ACSDataSource, ACSPublicCoverage, ACSIncome, BasicProblem, adult_filter\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from interpret import show\n",
        "from interpret.glassbox import LogisticRegression"
      ],
      "metadata": {
        "id": "z6InhiBwKXDD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define matrics\n",
        "\n",
        "# Independence\n",
        "def independence(y_hat, group):\n",
        "  \"\"\"\n",
        "  Computes an independence metric between two specific groups.\n",
        "\n",
        "  Args:\n",
        "    y_hat (np.ndarray): Classifier predictions.\n",
        "    group (np.ndarray): Array of indices corresponding to group membership.\n",
        "      For this assignment, we will focus on comparing groups 1 and 2.\n",
        "      These correspond to the 'White alone' and 'Black or African American'\n",
        "      groups. Note that one can also compare different combinations of groups.\n",
        "\n",
        "  Returns:\n",
        "    float: independence measure\n",
        "  \"\"\"\n",
        "  prob_a = np.sum(y_hat[group == 1])\n",
        "  prob_b = np.sum(y_hat[group == 2])\n",
        "\n",
        "  measure = prob_b / prob_a\n",
        "\n",
        "  return measure\n",
        "\n",
        "\n",
        "# Separation\n",
        "def separation(y_hat, y_true, group):\n",
        "  \"\"\"\n",
        "  Computes a separation metric between two specific groups.\n",
        "\n",
        "  Args:\n",
        "    y_hat  (np.ndarray): Classifier predictions.\n",
        "    y_true (np.ndarray): Data labels.\n",
        "    group  (np.ndarray): Array of indices corresponding to group membership.\n",
        "      For this assignment, we will focus on comparing groups 1 and 2.\n",
        "      These correspond to the 'White alone' and 'Black or African American'\n",
        "      groups. Note that one can also compare different combinations of groups.\n",
        "\n",
        "  Returns:\n",
        "    float: separation true positive\n",
        "    float: separation false positive\n",
        "  \"\"\"\n",
        "  group_a = (group == 1)\n",
        "  group_b = (group == 2)\n",
        "\n",
        "  tp_a = np.sum((y_hat == 1) & (y_true == 1) & group_a)\n",
        "  fp_a = np.sum((y_hat == 1) & (y_true == 0) & group_a)\n",
        "  tp_b = np.sum((y_hat == 1) & (y_true == 1) & group_b)\n",
        "  fp_b = np.sum((y_hat == 1) & (y_true == 0) & group_b)\n",
        "\n",
        "  tpr_a = tp_a / np.sum((y_true == 1) & group_a)\n",
        "  tpr_b = tp_b / np.sum((y_true == 1) & group_b)\n",
        "\n",
        "  fpr_a = fp_a / np.sum((y_true == 0) & group_a)\n",
        "  fpr_b = fp_b / np.sum((y_true == 0) & group_b)\n",
        "\n",
        "  separation_true_positive = tpr_b / tpr_a if tpr_b > 0 else np.inf\n",
        "  separation_false_positive = fpr_b / fpr_a if fpr_b > 0 else np.inf\n",
        "\n",
        "  return separation_true_positive, separation_false_positive\n",
        "\n",
        "\n",
        "# Sufficiency\n",
        "def sufficiency(y_hat, y_true, group):\n",
        "  \"\"\"\n",
        "  Computes a sufficiency metric between two specific groups.\n",
        "\n",
        "  Args:\n",
        "    y_hat  (np.ndarray): Classifier predictions.\n",
        "    y_true (np.ndarray): Data labels.\n",
        "    group  (np.ndarray): Array of indices corresponding to group membership.\n",
        "      For this assignment, we will focus on comparing groups 1 and 2.\n",
        "      These correspond to the 'White alone' and 'Black or African American'\n",
        "      groups. Note that one can also compare different combinations of groups.\n",
        "\n",
        "  Returns:\n",
        "    float: sufficiency metric\n",
        "  \"\"\"\n",
        "  group_a = (group == 1)\n",
        "  group_b = (group == 2)\n",
        "\n",
        "  ppv_a = np.sum((y_hat == 1) & (y_true == 1) & group_a) / np.sum((y_hat == 1) & group_a)\n",
        "  ppv_b = np.sum((y_hat == 1) & (y_true == 1) & group_b) / np.sum((y_hat == 1) & group_b)\n",
        "\n",
        "  if np.sum((y_hat == 1) & group_a) == 0:\n",
        "      ppv_a = np.inf\n",
        "  if np.sum((y_hat == 1) & group_b) == 0:\n",
        "      ppv_b = np.inf\n",
        "\n",
        "  sufficiency_positive = ppv_b / ppv_a if ppv_b > 0 else np.inf\n",
        "\n",
        "  return sufficiency_positive\n",
        "\n",
        "\n",
        "# SPD\n",
        "def spd(sensitive_attribute, dataset, predicted_labels, majority_class, minority_class):\n",
        "    \"\"\"\n",
        "    Calculate the Statistical Parity Difference (SPD) between majority and minority classes based on predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "    - sensitive_attribute (str): Name of the column representing the sensitive attribute.\n",
        "    - dataset (pd.DataFrame): The dataset containing the sensitive attribute and true outcome variable.\n",
        "    - predicted_labels (pd.Series): Predicted labels for the outcome variable.\n",
        "    - majority_class: Value representing the majority class in the sensitive attribute.\n",
        "    - minority_class: Value representing the minority class in the sensitive attribute.\n",
        "\n",
        "    Returns:\n",
        "    - spd (float): Statistical Parity Difference between majority and minority classes.\n",
        "    \"\"\"\n",
        "\n",
        "    prob_majority = np.sum(predicted_labels[dataset[sensitive_attribute] == majority_class]) / len(predicted_labels[dataset[sensitive_attribute] == majority_class])\n",
        "    prob_minority = np.sum(predicted_labels[dataset[sensitive_attribute] == minority_class]) / len(predicted_labels[dataset[sensitive_attribute] == minority_class])\n",
        "\n",
        "\n",
        "    spd_val = prob_minority - prob_majority\n",
        "\n",
        "    return spd_val\n",
        "\n",
        "\n",
        "# DI\n",
        "def di(sensitive_attribute, dataset, predicted_labels, majority_class, minority_class):\n",
        "    \"\"\"\n",
        "    Calculate the Disparate Impact (DI) between majority and minority classes based on predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "    - sensitive_attribute (str): Name of the column representing the sensitive attribute.\n",
        "    - dataset (pd.DataFrame): The dataset containing the sensitive attribute and true outcome variable.\n",
        "    - predicted_labels (pd.Series): Predicted labels for the outcome variable.\n",
        "    - majority_class: Value representing the majority class in the sensitive attribute.\n",
        "    - minority_class: Value representing the minority class in the sensitive attribute.\n",
        "\n",
        "    Returns:\n",
        "    - di (float): Disparate Impact between majority and minority classes.\n",
        "    \"\"\"\n",
        "\n",
        "    prob_majority = np.mean(predicted_labels[dataset[sensitive_attribute] == majority_class])\n",
        "    prob_minority = np.mean(predicted_labels[dataset[sensitive_attribute] == minority_class])\n",
        "\n",
        "    if prob_majority == 0:\n",
        "        return np.inf\n",
        "\n",
        "    di_val = prob_minority / prob_majority\n",
        "\n",
        "    return di_val\n",
        "\n",
        "\n",
        "# EOD\n",
        "def eod(sensitive_attribute, predictions, dataset, true_labels, majority_class, minority_class):\n",
        "    \"\"\"\n",
        "    Calculate the Equal Opportunity Difference (EOD) measure.\n",
        "\n",
        "    Parameters:\n",
        "    - sensitive_attribute: The column name of the sensitive attribute in the dataset.\n",
        "    - predictions: Predictions made by the model.\n",
        "    - dataset: The dataset containing the sensitive attribute and the outcome variable.\n",
        "    - outcome_variable: The column name of the outcome variable in the dataset.\n",
        "    - majority_class: The majority class label.\n",
        "    - minority_class: The minority class label.\n",
        "\n",
        "    Returns:\n",
        "    - eod_value: The Equal Opportunity Difference measure.\n",
        "    \"\"\"\n",
        "\n",
        "    majority = predictions[(dataset[sensitive_attribute] == majority_class) & (true_labels == 1)]\n",
        "    minority = predictions[(dataset[sensitive_attribute] == minority_class) & (true_labels == 1)]\n",
        "\n",
        "    tpr_majority = np.sum(majority) / len(majority)\n",
        "    tpr_minority = np.sum(minority) / len(minority)\n",
        "\n",
        "    eod_value = tpr_minority - tpr_majority\n",
        "\n",
        "    return eod_value\n",
        "\n",
        "# AAOD\n",
        "def aaod(sensitive_attribute, predictions, dataset, true_labels, majority_class, minority_class):\n",
        "    \"\"\"\n",
        "    Calculate the Average Absolute Odds Difference (AAOD) to measure bias.\n",
        "\n",
        "    Parameters:\n",
        "    - sensitive_attribute (str): The name of the sensitive attribute in the dataset.\n",
        "    - predictions (pd.Series): The predicted values.\n",
        "    - dataset (pd.DataFrame): The dataset containing the sensitive attribute, predictions, and outcome variable.\n",
        "    - outcome_variable (str): The name of the outcome variable in the dataset.\n",
        "    - majority_class (int): The label of the majority class.\n",
        "    - minority_class (int): The label of the minority class.\n",
        "\n",
        "    Returns:\n",
        "    - float: The calculated Average Absolute Odds Difference (AAOD).\n",
        "    \"\"\"\n",
        "    fp_majority = np.sum((predictions == 1) & (true_labels == 0) & (dataset[sensitive_attribute] == majority_class))\n",
        "    tn_majority = np.sum((predictions == 0) & (true_labels == 0) & (dataset[sensitive_attribute] == majority_class))\n",
        "    tp_majority = np.sum((predictions == 1) & (true_labels == 1) & (dataset[sensitive_attribute] == majority_class))\n",
        "    fn_majority = np.sum((predictions == 0) & (true_labels == 1) & (dataset[sensitive_attribute] == majority_class))\n",
        "    fpr_majority = fp_majority / (fp_majority + tn_majority) if (fp_majority + tn_majority) > 0 else 0\n",
        "    tpr_majority = tp_majority / (tp_majority + fn_majority) if (tp_majority + fn_majority) > 0 else 0\n",
        "\n",
        "    fp_minority = np.sum((predictions == 1) & (true_labels == 0) & (dataset[sensitive_attribute] == minority_class))\n",
        "    tn_minority = np.sum((predictions == 0) & (true_labels == 0) & (dataset[sensitive_attribute] == minority_class))\n",
        "    tp_minority = np.sum((predictions == 1) & (true_labels == 1) & (dataset[sensitive_attribute] == minority_class))\n",
        "    fn_minority = np.sum((predictions == 0) & (true_labels == 1) & (dataset[sensitive_attribute] == minority_class))\n",
        "    fpr_minority = fp_minority / (fp_minority + tn_minority) if (fp_minority + tn_minority) > 0 else 0\n",
        "    tpr_minority = tp_minority / (tp_minority + fn_minority) if (tp_minority + fn_minority) > 0 else 0\n",
        "\n",
        "    aaod_value = 0.5 * (abs(fpr_minority - fpr_majority) + abs(tpr_minority - tpr_majority))\n",
        "\n",
        "    return aaod_value"
      ],
      "metadata": {
        "id": "zDipOhiQKrXT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aPdn5s-N5-sh"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "df_dataset = pd.read_csv('dataset.csv', index_col=[0])\n",
        "df_dataset.head()\n",
        "\n",
        "df_score_1 = pd.read_csv('score_1.csv', index_col=[0])\n",
        "df_score_2 = pd.read_csv('score_2.csv', index_col=[0])\n",
        "df_score_3 = pd.read_csv('score_3.csv', index_col=[0])\n",
        "\n",
        "df_eval_1 = pd.read_csv('eval_1.csv', index_col=[0])\n",
        "df_eval_2 = pd.read_csv('eval_2.csv', index_col=[0])\n",
        "df_eval_3 = pd.read_csv('eval_3.csv', index_col=[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze with fairness metrics\n",
        "\n",
        "# For work authorization\n",
        "spd_work_1 = spd('Work authorization',df_dataset, df_eval_1, 1, 0)\n",
        "di_work_1 = di('Work authorization',df_dataset, df_eval_1, 1, 0)\n",
        "\n",
        "print(\"Result 1\")\n",
        "print(\"SPD: \", spd_work_1)\n",
        "print(\"DI: \", di_work_1)\n",
        "print()\n",
        "\n",
        "spd_work_2 = spd('Work authorization',df_dataset, df_eval_2, 1, 0)\n",
        "di_work_2 = di('Work authorization',df_dataset, df_eval_2, 1, 0)\n",
        "\n",
        "print(\"Result 2\")\n",
        "print(\"SPD: \", spd_work_2)\n",
        "print(\"DI: \", di_work_2)\n",
        "print()\n",
        "\n",
        "spd_work_3 = spd('Work authorization',df_dataset, df_eval_3, 1, 0)\n",
        "di_work_3 = di('Work authorization',df_dataset, df_eval_3, 1, 0)\n",
        "\n",
        "print(\"Result 3\")\n",
        "print(\"SPD: \", spd_work_3)\n",
        "print(\"DI: \", di_work_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlB25CnS4MKq",
        "outputId": "85ae18c9-ffb5-4499-e231-6b2823d7f4ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1\n",
            "SPD:  prediction   -0.030564\n",
            "dtype: float64\n",
            "DI:  0.8257179397744447\n",
            "\n",
            "Result 2\n",
            "SPD:  prediction   -0.005546\n",
            "dtype: float64\n",
            "DI:  0.9598286811401565\n",
            "\n",
            "Result 3\n",
            "SPD:  prediction   -0.026935\n",
            "dtype: float64\n",
            "DI:  0.8281290658339838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For work authorization\n",
        "spd_work_1 = spd('Work authorization',df_dataset, df_eval_1, 1, 0)\n",
        "di_work_1 = di('Work authorization',df_dataset, df_eval_1, 1, 0)\n",
        "\n",
        "print(\"Result 1\")\n",
        "print(\"SPD: \", spd_work_1)\n",
        "print(\"DI: \", di_work_1)\n",
        "print()\n",
        "\n",
        "spd_work_2 = spd('Work authorization',df_dataset, df_eval_2, 1, 0)\n",
        "di_work_2 = di('Work authorization',df_dataset, df_eval_2, 1, 0)\n",
        "\n",
        "print(\"Result 2\")\n",
        "print(\"SPD: \", spd_work_2)\n",
        "print(\"DI: \", di_work_2)\n",
        "print()\n",
        "\n",
        "spd_work_3 = spd('Work authorization',df_dataset, df_eval_3, 1, 0)\n",
        "di_work_3 = di('Work authorization',df_dataset, df_eval_3, 1, 0)\n",
        "\n",
        "print(\"Result 3\")\n",
        "print(\"SPD: \", spd_work_3)\n",
        "print(\"DI: \", di_work_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPNg-drhCbg-",
        "outputId": "57c37049-6d94-4a9b-87ab-9cfef05463e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1\n",
            "SPD:  prediction   -0.030564\n",
            "dtype: float64\n",
            "DI:  0.8257179397744447\n",
            "\n",
            "Result 2\n",
            "SPD:  prediction   -0.005546\n",
            "dtype: float64\n",
            "DI:  0.9598286811401565\n",
            "\n",
            "Result 3\n",
            "SPD:  prediction   -0.026935\n",
            "dtype: float64\n",
            "DI:  0.8281290658339838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # LIME\n",
        "# from sklearn.compose import ColumnTransformer\n",
        "# from sklearn.pipeline import Pipeline\n",
        "\n",
        "# d = pd.read_csv('dataset.csv')\n",
        "# e = pd.read_csv('eval_1.csv')\n",
        "\n",
        "# d = d.fillna('N/A', inplace=True)\n",
        "\n",
        "# model = LogisticRegression()\n",
        "\n",
        "# # Train your model\n",
        "# X = d\n",
        "# y = e['prediction']\n",
        "# model.fit(X, y)\n",
        "\n",
        "# explainer = LimeTabularExplainer(X.values,\n",
        "#                                 feature_names=X.columns,\n",
        "#                                 class_names=['eval'],\n",
        "#                                 verbose=True,\n",
        "#                                 mode='regression')\n",
        "\n",
        "# # Choose a random instance for explanation\n",
        "# i = np.random.randint(0, X.shape[0])\n",
        "# exp = explainer.explain_instance(X.values[i], model.predict, num_features=5)\n",
        "\n",
        "# # Visualize the explanation\n",
        "# exp.show_in_notebook(show_table=True)"
      ],
      "metadata": {
        "id": "YAkO_7tMF_cP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SHAP\n",
        "# import xgboost\n",
        "\n",
        "# # Train your model\n",
        "# X = df_dataset\n",
        "# # X = X.fillna(0, inplace=True)\n",
        "# X = X.astype('float')\n",
        "# y = df_eval_1['prediction']\n",
        "# X_encoded = pd.get_dummies(X)\n",
        "# model = xgboost.XGBClassifier().fit(X_encoded, y)"
      ],
      "metadata": {
        "id": "FgUVKwmsGAVT"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute SHAP values\n",
        "explainer = shap.Explainer(model, X_encoded)\n",
        "shap_values = explainer(X)\n",
        "shap.plots.beeswarm(shap_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "S8U3XgJhVBju",
        "outputId": "463751bb-4809-4d0e-ce6f-8bb480310933"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1441237ca898>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# compute SHAP values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeeswarm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_explainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, masker, link, algorithm, output_names, feature_names, linearize_link, seed, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tree\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m                 \u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinearize_link\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinearize_link\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"additive\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdditiveExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, model_output, feature_perturbation, feature_names, approximate, link, linearize_link)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_perturbation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_perturbation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpected_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreeEnsemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m#self.model_output = self.model.model_output # this allows the TreeEnsemble to translate model outputs types by how it loads the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, data_missing, model_output)\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             self._set_xgboost_model_attributes(\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m                 \u001b[0mdata_missing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m_set_xgboost_model_attributes\u001b[0;34m(self, data, data_missing, objective_name_map, tree_output_name_map)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBTreeModelLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective_name_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36mget_trees\u001b[0;34m(self, data, data_missing)\u001b[0m\n\u001b[1;32m   1990\u001b[0m                 \u001b[0;34m\"node_sample_weight\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_hess\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1991\u001b[0m             }\n\u001b[0;32m-> 1992\u001b[0;31m             \u001b[0mtrees\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSingleTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/shap/explainers/_tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tree, normalize, scaling, data, data_missing)\u001b[0m\n\u001b[1;32m   1721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata_missing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_sample_weight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             _cext.dense_tree_update_weights(\n\u001b[0m\u001b[1;32m   1724\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('O') to dtype('float64') according to the rule 'safe'"
          ]
        }
      ]
    }
  ]
}